{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Przygotowanie tekstów do analizy za pomocą metody topic modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W naszym eksperymencie posłużymy się dostępnymi za darmo korpusami - zbiorami tekstów - ELTeC (European Literary Text Collection), utworzonymi m.in. przez nasz zespół w ramach COST Action Distant Reading for European Literary History (CA16204, https://distant-reading.net)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak wiele zasobów literackich, korpusy ELTeC są dostępne w formacie XML, który pozwala na zachowanie różnych informacji m.in. o formie tekstu - podziale na akapity, rozdziały itp. Tymczasem większość metod text miningu preferuje korzystanie z możliwie najprostszych formatów tekstu, przede wszystkim tzw. \"plain text\" (czyli .txt). Metody modelowania pojęciowego (topic modeling) nie są tu wyjątkiem. Co więcej, ponieważ metoda ta była wynaleziona na tworzenia słów kluczy i tagów na podstawie krótkich tekstów, np. abstraktów, wymaga ona także podziału na stosunkowo krótkie fragmenty tekstu - a więc korpus powieści musimy odpowiednio \"pokroić\". W naszym przypadku, pokroimy go na akapity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytujemy potrzebne nam biblioteki\n",
    "import lxml\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from simplemma import text_lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "czech_path = os.listdir(\"ELTeC_originals/Cze/level1\")\n",
    "polish_path = os.listdir(\"ELTeC_originals/Pol/level1\")\n",
    "cze_output_path = os.listdir(\"plain_texts/Cze/\")\n",
    "pol_output_path = os.listdir(\"plain_texts/Pol/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy sprawdzić, że w zmiennej faktycznie znajduje się lista naszych plików wyświetlając pięć pierwszych elementów w ten sposób:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając listę plików przechodzimy do sedna - chcemy usunąć niepotrzebne nam znaczniki XML. Zrobimy to używając funkcji etree z biblioteki lxml. Co istotne, ponieważ oba języki w naszym studium mają znaki diakrytyczne, chcemy się upewnić, że przy ich przetwarzaniu będziemy trzymać się kodowania UTF-8, które poprawnie je zapisuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(\"ELTeC_originals/Cze/level1\"):\n",
    "    for name in files:\n",
    "        file_path = os.path.join(path,name)\n",
    "# Wczytujemy plik i od razu \"wrzucamy go do zupy\" zastrzegając, że to xml\n",
    "        with open(file_path) as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "# Znajdujemy tekst oznaczony paragrafami (<p>) w części zawierającej tekst (text)            \n",
    "        tags = soup.find('text').find_all('p')\n",
    "# Dodajemy znak nowej linii (\\n) pomiędzy wykryte paragrafy\n",
    "        stripped = (\"\\n\").join([t.text for t in tags])\n",
    "# Przygotowujemy nową nazwę pliku  \n",
    "            new_name = os.path.splitext(name)[0]\n",
    "            new_path = os.path.join(\"plain_texts/Cze/\",new_name+\".txt\")\n",
    "# I zapisujemy\n",
    "            with open(new_path, 'w+') as output:\n",
    "                output.write(stripped)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(\"ELTeC_originals/Pol/level1\"):\n",
    "    for name in files:\n",
    "        file_path = os.path.join(path,name)\n",
    "# Wczytujemy plik i od razu \"wrzucamy go do zupy\" zastrzegając, że to xml\n",
    "        with open(file_path) as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "# Znajdujemy tekst oznaczony paragrafami (<p>) w części zawierającej tekst (text)\n",
    "        tags = soup.find('text').find_all('p')\n",
    "# Dodajemy znak nowej linii (\\n) pomiędzy wykryte paragrafy\n",
    "        stripped = (\"\\n\").join([t.text for t in tags])\n",
    "# Przygotowujemy nową nazwę pliku  \n",
    "        new_name = os.path.splitext(name)[0]\n",
    "        new_path = os.path.join(\"plain_texts/Pol/\", new_name+\".txt\")\n",
    "# I zapisujemy\n",
    "        with open(new_path, 'w+') as output:\n",
    "                output.write(stripped) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdy mamy już pliki z czystym tekstem, chcemy poddać je lemmatyzacji - czyli sprowadzić je do formy podstawowej. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(\"plain_texts/Cze\"):\n",
    "    for name in files:\n",
    "        file_path = os.path.join(path,name)\n",
    "        with open(file_path) as file:\n",
    "            text = file.read()\n",
    "# Aby zachować strukturę podziału na paragrafy, lematyzację wykonujemy na paragrafach       \n",
    "        paragraphs = text.split(\"\\n\")\n",
    "        result = [text_lemmatizer(p, lang='cs') for p in paragraphs]\n",
    "        \n",
    "# Zlematyzowane paragrafy chcemy połączyć w jeden tekst. Aby zachować informację o końcu paragrafu, w miejsce znaku końca linii podstawiamy znacznik EOL.\n",
    "        joined_lines = [\"\\n\".join(x) for x in result]\n",
    "        joined_text = \"\\nEOL\\n\".join(joined_lines) \n",
    "# Przygotowujemy nową nazwę pliku            \n",
    "        new_name = os.path.splitext(name)[0]\n",
    "        new_path = os.path.join(\"lemmatized_texts/Cze/\", new_name+\"_lemmatized.txt\")\n",
    "# I zapisujemy\n",
    "        with open(new_path, 'w+') as output:\n",
    "            output.write(joined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(\"plain_texts/Pol\"):\n",
    "    for name in files:\n",
    "        file_path = os.path.join(path,name)\n",
    "        \n",
    "        with open(file_path) as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        paragraphs = text.split(\"\\n\")\n",
    "        result = [text_lemmatizer(p, lang=\"pl\") for p in paragraphs]\n",
    "        joined_lines = [\"\\n\".join(x) for x in result]\n",
    "        joined_text = \"\\nEOL\\n\".join(joined_lines)\n",
    "        \n",
    "        new_name = os.path.splitext(name)[0]\n",
    "        new_path = os.path.join(\"lemmatized_texts/Pol/\", new_name+\"_lemmatized.txt\")\n",
    "        with open(new_path, 'w+') as output:\n",
    "            output.write(joined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gotowe zlematyzowane teksty możemy teraz poddać wybranej metodzie analizy - w naszym wypadku topic modelingowi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
